# Calculate-Information-Gain-for-Machine-Learning



Information Gain Formula
Note that the child groups are weighted equally in this case since they're both the same size, for all splits. In general, the average entropy for the child groups will need to be a weighted average, based on the number of cases in each child group. That is, for mm items in the first child group and nn items in the second child group, the information gain is:


<img width="476" alt="Screen Shot 2021-05-24 at 2 47 30 PM" src="https://user-images.githubusercontent.com/25523756/119928517-92f81780-bf30-11eb-88f2-bdc7ce31fba2.png">




<img width="476" alt="Screen Shot 2021-05-24 at 2 47 30 PM" src="https://user-images.githubusercontent.com/25523756/119928579-aefbb900-bf30-11eb-8027-e894306e913e.png">


<img width="1054" alt="Screen Shot 2021-05-27 at 9 16 32 PM" src="https://user-images.githubusercontent.com/25523756/119928651-d3f02c00-bf30-11eb-92df-56b2fc268ccc.png">

<img width="1004" alt="Screen Shot 2021-05-27 at 9 17 21 PM" src="https://user-images.githubusercontent.com/25523756/119928701-f3875480-bf30-11eb-9a4d-690fb893e858.png">



